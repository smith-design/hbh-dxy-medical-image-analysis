"""
åŸºäº BiomedCLIP çš„çš®è‚¤ç—…å˜åˆ†ç±»æ¨¡å‹
ä½¿ç”¨é¢„è®­ç»ƒçš„åŒ»å­¦è§†è§‰æ¨¡å‹è¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»
"""

import torch
import torch.nn as nn
from transformers import AutoModel, AutoProcessor
from PIL import Image
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ç–¾ç—…ç±»å‹æ˜ å°„
DISEASE_CLASSES = {
    0: 'akiec',  # å…‰åŒ–æ€§è§’åŒ–ç—…å’Œä¸Šçš®å†…ç™Œ
    1: 'bcc',    # åŸºåº•ç»†èƒç™Œ
    2: 'bkl',    # è‰¯æ€§è§’åŒ–ç—…å˜
    3: 'df',     # çš®è‚¤çº¤ç»´ç˜¤
    4: 'mel',    # é»‘è‰²ç´ ç˜¤
    5: 'nv',     # é»‘è‰²ç´ ç—£
    6: 'vasc'    # è¡€ç®¡ç—…å˜
}

DISEASE_NAMES = {
    'akiec': 'å…‰åŒ–æ€§è§’åŒ–ç—…å’Œä¸Šçš®å†…ç™Œ',
    'bcc': 'åŸºåº•ç»†èƒç™Œ',
    'bkl': 'è‰¯æ€§è§’åŒ–ç—…å˜',
    'df': 'çš®è‚¤çº¤ç»´ç˜¤',
    'mel': 'é»‘è‰²ç´ ç˜¤',
    'nv': 'é»‘è‰²ç´ ç—£',
    'vasc': 'è¡€ç®¡ç—…å˜'
}

class SkinLesionClassifier(nn.Module):
    """çš®è‚¤ç—…å˜åˆ†ç±»å™¨"""

    def __init__(self, num_classes=7, feature_dim=768):
        super().__init__()

        # ä½¿ç”¨ BiomedCLIP ä½œä¸ºç‰¹å¾æå–å™¨
        # å¦‚æœ BiomedCLIP ä¸å¯ç”¨ï¼Œä½¿ç”¨ CLIP æˆ– DINOv2
        self.feature_extractor = None
        self.processor = None

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )

    def load_feature_extractor(self, model_name="microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"):
        """åŠ è½½ç‰¹å¾æå–å™¨"""
        print(f"ğŸ“¦ åŠ è½½ç‰¹å¾æå–å™¨: {model_name}")
        
        # 1. å°è¯•ä»æœ¬åœ°ç¼“å­˜æˆ– Hugging Face åŠ è½½ BiomedCLIP
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°ç¼“å­˜çš„ BiomedCLIP (Standard location)
            home_dir = Path.home()
            cache_dir = home_dir / ".cache" / "huggingface" / "hub"
            print(f"ğŸ” æ£€æŸ¥æ¨¡å‹ç¼“å­˜: {cache_dir}")
            
            from open_clip import create_model_from_pretrained
            
            # ä½¿ç”¨ local_files_only=True å¦‚æœä½ æƒ³å¼ºåˆ¶ç¦»çº¿ï¼Œä½†è¿™é‡Œæˆ‘ä»¬å…ˆå°è¯•æ­£å¸¸åŠ è½½
            # å¦‚æœç½‘ç»œä¸å¥½ï¼Œopen_clip å¯èƒ½ä¼šå¡ä½ï¼Œä½†æˆ‘ä»¬ä¹Ÿæ²¡åŠæ³•ç›´æ¥è·³è¿‡å®ƒå»æ£€æµ‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            # é™¤éæˆ‘ä»¬æ‰‹åŠ¨ç®¡ç†ä¸‹è½½ã€‚
            
            print(f"â³ å°è¯•åŠ è½½ BiomedCLIP (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ä¸‹è½½)...")
            model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
            self.feature_extractor = model.visual
            self.processor = preprocess
            print("âœ… BiomedCLIP åŠ è½½æˆåŠŸ")
            return

        except Exception as e:
            print(f"âš ï¸  BiomedCLIP åŠ è½½é‡åˆ°é—®é¢˜: {e}")
        
        # 2. å¦‚æœ BiomedCLIP å¤±è´¥ï¼Œå°è¯•åŠ è½½ DINOv2 ä½œä¸ºå¤‡é€‰ (æ›´å¿«ï¼Œä¸éœ€è¦ open_clip)
        print("ğŸ”„ åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹: DINOv2 (facebook/dinov2-small)")
        try:
            from transformers import AutoModel, AutoImageProcessor
            model = AutoModel.from_pretrained("facebook/dinov2-small")
            self.feature_extractor = model
            self.processor = AutoImageProcessor.from_pretrained("facebook/dinov2-small")
            print("âœ… DINOv2 åŠ è½½æˆåŠŸ (ä½œä¸ºå¤‡é€‰)")
        except Exception as e2:
            print(f"âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ã€‚è¯¦ç»†é”™è¯¯: {e2}")
            # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å®ƒç»§ç»­è¿è¡Œï¼Œåªæ˜¯æ¨¡å‹å¯èƒ½æ— æ³•å·¥ä½œï¼Œæˆ–è€…ä½¿ç”¨éšæœºæƒé‡
            print("âš ï¸ ä½¿ç”¨æœªåˆå§‹åŒ–çš„æ¨¡å‹ç»“æ„è¿è¡Œ (ä»…ä¾›æµ‹è¯•)")

    def extract_features(self, images):
        """æå–å›¾åƒç‰¹å¾"""
        with torch.no_grad():
            # æ£€æŸ¥ self.processor çš„ç±»å‹æ¥å†³å®šå¦‚ä½•å¤„ç†
            is_transform_pipeline = False
            if hasattr(self.processor, 'transforms'): 
                is_transform_pipeline = True
            elif hasattr(self.processor, 'preprocess'): 
                 is_transform_pipeline = True
            elif callable(self.processor) and not hasattr(self.processor, 'from_pretrained'):
                 is_transform_pipeline = True
                 
            if is_transform_pipeline:
                # 1. BiomedCLIP (open_clip) 
                processed_tensors = []
                for img in images:
                    processed_tensors.append(self.processor(img))
                processed = torch.stack(processed_tensors)
                features = self.feature_extractor(processed)
            else:
                # 2. Hugging Face DINOv2
                processed = self.processor(images=images, return_tensors="pt")
                if 'pixel_values' in processed:
                    processed = processed['pixel_values']
                features = self.feature_extractor(processed)

            # å¤„ç† features æå–
            if isinstance(features, tuple):
                features = features[0]
            
            if hasattr(features, 'pooler_output') and features.pooler_output is not None:
                features = features.pooler_output
            elif hasattr(features, 'last_hidden_state'):
                features = features.last_hidden_state[:, 0]  # CLS token
            
            # --- å…³é”®ä¿®å¤ï¼šç»´åº¦åŒ¹é… ---
            # DINOv2 è¾“å‡ºç»´åº¦æ˜¯ 384 (small) æˆ– 768 (base)
            # BiomedCLIP é€šå¸¸æ˜¯ 512 æˆ– 768
            # åˆ†ç±»å™¨è¾“å…¥å±‚å®šä¹‰ä¸º 768 (feature_dim)
            # é”™è¯¯æç¤ºï¼šmat1=1x512, mat2=768x512 -> è¯´æ˜ BiomedCLIP è¾“å‡ºäº† 512ï¼Œä½†åˆ†ç±»å™¨æœŸæœ› 768
            # æˆ–è€…åä¹‹ã€‚
            
            # åŠ¨æ€è°ƒæ•´ï¼šå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œ padding æˆ– projection
            # ä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•è®­ç»ƒ projectionï¼Œåªèƒ½ padding
            
            current_dim = features.shape[1]
            target_dim = self.classifier[0].in_features
            
            if current_dim != target_dim:
                # print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: è¾“å‡º {current_dim}, æœŸæœ› {target_dim}. å°è¯•è°ƒæ•´...")
                if current_dim < target_dim:
                    # Pad with zeros
                    padding = torch.zeros(features.shape[0], target_dim - current_dim).to(features.device)
                    features = torch.cat([features, padding], dim=1)
                else:
                    # Truncate (ä¸å¤ªç†æƒ³ä½†èƒ½è·‘)
                    features = features[:, :target_dim]
                    
            return features

    def forward(self, images):
        """å‰å‘ä¼ æ’­"""
        features = self.extract_features(images)
        logits = self.classifier(features)
        return logits

def train_classifier(data_dir, output_dir, epochs=10, batch_size=32, lr=1e-3):
    """è®­ç»ƒåˆ†ç±»å™¨"""

    print("ğŸš€ å¼€å§‹è®­ç»ƒçš®è‚¤ç—…å˜åˆ†ç±»å™¨...")

    # åŠ è½½æ•°æ®
    data_dir = Path(data_dir)
    metadata_path = data_dir.parent.parent / "datasets" / "archive (6)" / "HAM10000_metadata.csv"

    import pandas as pd
    metadata = pd.read_csv(metadata_path)

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    label_map = {v: k for k, v in DISEASE_CLASSES.items()}
    metadata['label'] = metadata['dx'].map(label_map)    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    from sklearn.model_selection import train_test_split
    train_df, val_df = train_test_split(metadata, test_size=0.2, stratify=metadata['label'], random_state=42)

    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_df)} æ ·æœ¬")

    # åˆ›å»ºæ¨¡å‹
    model = SkinLesionClassifier()
    model.load_feature_extractor()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    model = model.to(device)

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{epochs}")

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        # ç®€å•çš„æ‰¹æ¬¡è®­ç»ƒï¼ˆå®é™…åº”è¯¥ç”¨ DataLoaderï¼‰
        for idx in tqdm(range(0, len(train_df), batch_size), desc="Training"):
            batch_df = train_df.iloc[idx:idx+batch_size]

            # åŠ è½½å›¾åƒ
            images = []
            labels = []
            for _, row in batch_df.iterrows():
                img_id = row['image_id']
                img_path_1 = data_dir.parent.parent / "datasets" / "archive (6)" / "HAM10000_images_part_1" / f"{img_id}.jpg"
                img_path_2 = data_dir.parent.parent / "datasets" / "archive (6)" / "HAM10000_images_part_2" / f"{img_id}.jpg"

                if img_path_1.exists():
                    img = Image.open(img_path_1).convert('RGB')
                elif img_path_2.exists():
                    img = Image.open(img_path_2).convert('RGB')
                else:
                    continue

                images.append(img)
                labels.append(row['label'])

            if not images:
                continue

            labels = torch.tensor(labels, dtype=torch.long).to(device)

            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()

        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / (len(train_df) // batch_size)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for idx in tqdm(range(0, len(val_df), batch_size), desc="Validation"):
                batch_df = val_df.iloc[idx:idx+batch_size]

                images = []
                labels = []
                for _, row in batch_df.iterrows():
                    img_id = row['image_id']
                    img_path_1 = data_dir.parent.parent / "datasets" / "archive (6)" / "HAM10000_images_part_1" / f"{img_id}.jpg"
                    img_path_2 = data_dir.parent.parent / "datasets" / "archive (6)" / "HAM10000_images_part_2" / f"{img_id}.jpg"

                    if img_path_1.exists():
                        img = Image.open(img_path_1).convert('RGB')
                    elif img_path_2.exists():
                        img = Image.open(img_path_2).convert('RGB')
                    else:
                        continue

                    images.append(img)
                    labels.append(row['label'])

                if not images:
                    continue

                labels = torch.tensor(labels, dtype=torch.long).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_acc = 100. * val_correct / val_total
        avg_val_loss = val_loss / (len(val_df) // batch_size)

        # è®°å½•å†å²
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)

        print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"éªŒè¯æŸå¤±: {avg_val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            output_path = Path(output_dir) / "best_model.pth"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            torch.save({
             'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'history': history
            }, output_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = Path(output_dir) / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    return model, history

if __name__ == "__main__":
    # è®­ç»ƒåˆ†ç±»å™¨
    model, history = train_classifier(
        data_dir="data/processed",
        output_dir="models/biomedclip_classifier",
        epochs=10,
        batch_size=16,
        lr=1e-3
    )
