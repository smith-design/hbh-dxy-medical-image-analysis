"""
HAM10000 æ•°æ®é¢„å¤„ç†è„šæœ¬
å°† HAM10000 æ•°æ®é›†è½¬æ¢ä¸º LLaMA-Factory æ”¯æŒçš„å¤šæ¨¡æ€è®­ç»ƒæ ¼å¼
"""

import os
import json
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
import shutil
from tqdm import tqdm

# ç–¾ç—…ç±»å‹æ˜ å°„
DISEASE_MAPPING = {
    'akiec': 'å…‰åŒ–æ€§è§’åŒ–ç—…å’Œä¸Šçš®å†…ç™Œ (Actinic Keratoses and Intraepithelial Carcinoma)',
    'bcc': 'åŸºåº•ç»†èƒç™Œ (Basal Cell Carcinoma)',
    'bkl': 'è‰¯æ€§è§’åŒ–ç—…å˜ (Benign Keratosis)',
    'df': 'çš®è‚¤çº¤ç»´ç˜¤ (Dermatofibroma)',
    'mel': 'é»‘è‰²ç´ ç˜¤ (Melanoma)',
    'nv': 'é»‘è‰²ç´ ç—£ (Melanocytic Nevi)',
    'vasc': 'è¡€ç®¡ç—…å˜ (Vascular Lesions)'
}

# ç–¾ç—…æè¿°æ¨¡æ¿
DISEASE_DESCRIPTIONS = {
    'akiec': 'è¿™æ˜¯ä¸€ç§å…‰åŒ–æ€§è§’åŒ–ç—…æˆ–ä¸Šçš®å†…ç™Œï¼Œé€šå¸¸ç”±é•¿æœŸæ—¥æ™’å¼•èµ·ï¼Œè¡¨ç°ä¸ºç²—ç³™ã€é³çŠ¶çš„çš®è‚¤æ–‘å—ã€‚',
    'bcc': 'è¿™æ˜¯åŸºåº•ç»†èƒç™Œï¼Œæœ€å¸¸è§çš„çš®è‚¤ç™Œç±»å‹ï¼Œé€šå¸¸ç”Ÿé•¿ç¼“æ…¢ï¼Œå¾ˆå°‘è½¬ç§»ï¼Œä½†éœ€è¦åŠæ—¶æ²»ç–—ã€‚',
    'bkl': 'è¿™æ˜¯è‰¯æ€§è§’åŒ–ç—…å˜ï¼ŒåŒ…æ‹¬è„‚æº¢æ€§è§’åŒ–ç—…ç­‰ï¼Œé€šå¸¸æ— å®³ä½†å¯èƒ½å½±å“ç¾è§‚ã€‚',
    'df': 'è¿™æ˜¯çš®è‚¤çº¤ç»´ç˜¤ï¼Œä¸€ç§è‰¯æ€§çš„çº¤ç»´ç»„ç»‡å¢ç”Ÿï¼Œé€šå¸¸è¡¨ç°ä¸ºåšç¡¬çš„å°ç»“èŠ‚ã€‚',
    'mel': 'è¿™æ˜¯é»‘è‰²ç´ ç˜¤ï¼Œæœ€å±é™©çš„çš®è‚¤ç™Œç±»å‹ï¼Œå¯èƒ½å¿«é€Ÿç”Ÿé•¿å’Œè½¬ç§»ï¼Œéœ€è¦ç´§æ€¥åŒ»ç–—å…³æ³¨ã€‚',
    'nv': 'è¿™æ˜¯é»‘è‰²ç´ ç—£ï¼Œä¿—ç§°ç—£æˆ–ç—¦å­ï¼Œé€šå¸¸æ˜¯è‰¯æ€§çš„ï¼Œä½†éœ€è¦ç›‘æµ‹å˜åŒ–ã€‚',
    'vasc': 'è¿™æ˜¯è¡€ç®¡ç—…å˜ï¼ŒåŒ…æ‹¬è¡€ç®¡ç˜¤ç­‰ï¼Œç”±è¡€ç®¡å¼‚å¸¸å¢ç”Ÿå¼•èµ·ã€‚'
}

def prepare_data():
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""

    # è·¯å¾„é…ç½®
    base_dir = Path(__file__).parent.parent
    dataset_dir = base_dir / "datasets" / "archive (6)"
    output_dir = base_dir / "data" / "processed"
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºå›¾åƒç›®å½•
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)

    print("ğŸ“Š è¯»å–å…ƒæ•°æ®...")
    metadata = pd.read_csv(dataset_dir / "HAM10000_metadata.csv")

    print(f"âœ… æ€»æ ·æœ¬æ•°: {len(metadata)}")
    print(f"âœ… ç–¾ç—…ç±»å‹åˆ†å¸ƒ:")
    print(metadata['dx'].value_counts())

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    data_samples = []

    print("\nğŸ”„ å¤„ç†å›¾åƒå’Œç”Ÿæˆè®­ç»ƒæ ·æœ¬...")
    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):
        image_id = row['image_id']
        lesion_id = row['lesion_id']
        dx = row['dx']
        dx_type = row['dx_type']
        age = row['age']
        sex = row['sex']
        localization = row['localization']

        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        image_path_1 = dataset_dir / "HAM10000_images_part_1" / f"{image_id}.jpg"
        image_path_2 = dataset_dir / "HAM10000_images_part_2" / f"{image_id}.jpg"

        if image_path_1.exists():
            source_image = image_path_1
        elif image_path_2.exists():
            source_image = image_path_2
        else:
            print(f"âš ï¸  å›¾åƒæœªæ‰¾åˆ°: {image_id}")
            continue

        # å¤åˆ¶å›¾åƒåˆ°å¤„ç†ç›®å½•
        target_image = images_dir / f"{image_id}.jpg"
        if not target_image.exists():
            shutil.copy2(source_image, target_image)

        # ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬
        disease_name = DISEASE_MAPPING[dx]
        disease_desc = DISEASE_DESCRIPTIONS[dx]

        # æ ·æœ¬1: ç–¾ç—…åˆ†ç±»
        sample1 = {
            "messages": [
                {
                    "role": "user",
                    "content": f"<image>è¯·åˆ†æè¿™å¼ çš®è‚¤ç—…å˜å›¾åƒï¼Œåˆ¤æ–­ç—…å˜ç±»å‹ã€‚"
                },
                {
                    "role": "assistant",
                    "content": f"æ ¹æ®å›¾åƒåˆ†æï¼Œè¿™æ˜¯{disease_name}ã€‚{disease_desc}"
                }
            ],
            "images": [f"images/{image_id}.jpg"]
        }

        # æ ·æœ¬2: è¯¦ç»†è¯Šæ–­
        patient_info = f"æ‚£è€…ä¿¡æ¯ï¼š{sex}ï¼Œ{age}å²ï¼Œç—…å˜ä½ç½®ï¼š{localization}"
        sample2 = {
            "messages": [
                {
                    "role": "user",
                    "content": f"<image>è¯·æä¾›è¯¦ç»†çš„è¯Šæ–­åˆ†æã€‚{patient_info}ã€‚"
                },
                {
                    "role": "assistant",
                    "content": f"è¯Šæ–­ç»“æœï¼š{disease_name}\n\n{disease_desc}\n\n{patient_info}\nè¯Šæ–­æ–¹æ³•ï¼š{dx_type}\n\nå»ºè®®ï¼šè¯·å’¨è¯¢çš®è‚¤ç§‘åŒ»ç”Ÿè¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚"
                }
            ],
            "images": [f"images/{image_id}.jpg"]
        }

        # æ ·æœ¬3: ç®€çŸ­é—®ç­”
        sample3 = {
            "messages": [
                {
                    "role": "user",
                    "content": f"<image>è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„çš®è‚¤ç—…å˜ï¼Ÿ"
                },
                {
                    "role": "assistant",
                    "content": f"{disease_name}"
                }
            ],
            "images": [f"images/{image_id}.jpg"]
        }

        data_samples.extend([sample1, sample2, sample3])

    print(f"\nâœ… ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ€»æ•°: {len(data_samples)}")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = train_test_split(
        data_samples,
        test_size=0.1,
        random_state=42
    )

    print(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_samples)}")
    print(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {len(val_samples)}")

    # ä¿å­˜ä¸º JSON æ ¼å¼
    train_file = output_dir / "train.json"
    val_file = output_dir / "val.json"

    with open(train_file, 'w', encoding='utf-8') as f:
        json.dump(train_samples, f, ensure_ascii=False, indent=2)

    with open(val_file, 'w', encoding='utf-8') as f:
        json.dump(val_samples, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {train_file}")
    print(f"âœ… éªŒè¯æ•°æ®å·²ä¿å­˜åˆ°: {val_file}")

    # ç”Ÿæˆæ•°æ®é›†é…ç½®æ–‡ä»¶
    dataset_info = {
        "ham10000_skin_lesion": {
            "file_name": "train.json",
            "formatting": "sharegpt",
            "columns": {
                "messages": "messages",
                "images": "images"
            },
            "tags": {
                "role_tag": "role",
                "content_tag": "content",
                "user_tag": "user",
                "assistant_tag": "assistant"
            }
        }
    }

    dataset_info_file = output_dir / "dataset_info.json"
    with open(dataset_info_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ•°æ®é›†é…ç½®å·²ä¿å­˜åˆ°: {dataset_info_file}")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š")
    print("="*50)
    print(f"åŸå§‹å›¾åƒæ•°é‡: {len(metadata)}")
    print(f"ç”Ÿæˆè®­ç»ƒæ ·æœ¬: {len(data_samples)}")
    print(f"è®­ç»ƒé›†: {len(train_samples)}")
    print(f"éªŒè¯é›†: {len(val_samples)}")
    print(f"å›¾åƒå­˜å‚¨è·¯å¾„: {images_dir}")
    print("="*50)

if __name__ == "__main__":
    prepare_data()
