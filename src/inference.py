"""
æ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºæµ‹è¯•å¾®è°ƒåçš„ Qwen2-VL æ¨¡å‹
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
from pathlib import Path
import sys

def load_model(model_path=None, base_model="Qwen/Qwen2-VL-7B-Instruct"):
    """åŠ è½½æ¨¡å‹"""

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")

    if model_path and Path(model_path).exists():
        print(f"ğŸ“¦ åŠ è½½å¾®è°ƒæ¨¡å‹: {model_path}")
        from peft import PeftModel

        # åŠ è½½åŸºåº§æ¨¡å‹
        base = Qwen2VLForConditionalGeneration.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
            device_map="auto"
        )

        # åŠ è½½ LoRA æƒé‡
        model = PeftModel.from_pretrained(base, model_path)
        model = model.merge_and_unload()
    else:
        print(f"ğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹: {base_model}")
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
            device_map="auto"
        )

    processor = AutoProcessor.from_pretrained(base_model)
    model.eval()

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, processor, device

def diagnose_image(image_path, model, processor, device, question=None):
    """è¯Šæ–­å›¾åƒ"""

    if question is None:
        question = "è¯·åˆ†æè¿™å¼ çš®è‚¤ç—…å˜å›¾åƒï¼Œåˆ¤æ–­ç—…å˜ç±»å‹å¹¶æä¾›è¯¦ç»†çš„è¯Šæ–­å»ºè®®ã€‚"

    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    print(f"ğŸ“¸ å›¾åƒå¤§å°: {image.size}")

    # å‡†å¤‡è¾“å…¥
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": question}
            ]
        }
    ]

    # å¤„ç†è¾“å…¥
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    ).to(device)

    # ç”Ÿæˆè¯Šæ–­
    print("ğŸ” æ­£åœ¨åˆ†æ...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False,
            temperature=0.7,
            top_p=0.9
        )

    # è§£ç è¾“å‡º
    generated_text = processor.batch_decode(
        outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    # æå–åŠ©æ‰‹å›å¤
    diagnosis = generated_text.split("assistant\n")[-1].strip()

    return diagnosis

def main():
    """ä¸»å‡½æ•°"""

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python src/inference.py <å›¾åƒè·¯å¾„> [é—®é¢˜]")
        print("ç¤ºä¾‹: python src/inference.py test_image.jpg")
        sys.exit(1)

    image_path = sys.argv[1]
    question = sys.argv[2] if len(sys.argv) > 2 else None

    if not Path(image_path).exists():
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        sys.exit(1)

    # æ¨¡å‹è·¯å¾„
    base_dir = Path(__file__).parent.parent
    model_path = base_dir / "models" / "qwen2vl_ham10000_lora"

    # åŠ è½½æ¨¡å‹
    model, processor, device = load_model(
        model_path=str(model_path) if model_path.exists() else None
    )

    # è¯Šæ–­
    diagnosis = diagnose_image(image_path, model, processor, device, question)

    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ“‹ è¯Šæ–­ç»“æœ")
    print("="*60)
    print(diagnosis)
    print("="*60)

if __name__ == "__main__":
    main()
