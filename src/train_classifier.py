"""
åŸºäºåŒ»å­¦é¢„è®­ç»ƒæ¨¡å‹çš„çš®è‚¤ç—…å˜åˆ†ç±»å™¨
ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼šåŒ»å­¦é¢„è®­ç»ƒæ¨¡å‹ â†’ HAM10000 å¾®è°ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import json
import timm  # PyTorch Image Models - åŒ…å«å„ç§é¢„è®­ç»ƒæ¨¡å‹

# ç–¾ç—…ç±»åˆ«
DISEASE_CLASSES = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']
DISEASE_NAMES = {
    'akiec': 'å…‰åŒ–æ€§è§’åŒ–ç—…å’Œä¸Šçš®å†…ç™Œ',
    'bcc': 'åŸºåº•ç»†èƒç™Œ',
    'bkl': 'è‰¯æ€§è§’åŒ–ç—…å˜',
    'df': 'çš®è‚¤çº¤ç»´ç˜¤',
    'mel': 'é»‘è‰²ç´ ç˜¤',
    'nv': 'é»‘è‰²ç´ ç—£',
    'vasc': 'è¡€ç®¡ç—…å˜'
}

class HAM10000Dataset(Dataset):
    """HAM10000 æ•°æ®é›†"""

    def __init__(self, metadata_df, img_dir1, img_dir2, transform=None):
        self.metadata = metadata_df.reset_index(drop=True)
        self.img_dir1 = Path(img_dir1)
        self.img_dir2 = Path(img_dir2)
        self.transform = transform
        self.label_map = {cls: idx for idx, cls in enumerate(DISEASE_CLASSES)}

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        row = self.metadata.iloc[idx]
        img_id = row['image_id']
        label = self.label_map[row['dx']]

        # æŸ¥æ‰¾å›¾åƒ
        img_path1 = self.img_dir1 / f"{img_id}.jpg"
        img_path2 = self.img_dir2 / f"{img_id}.jpg"

        if img_path1.exists():
            image = Image.open(img_path1).convert('RGB')
        elif img_path2.exists():
            image = Image.open(img_path2).convert('RGB')
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆå›¾åƒï¼ˆé¿å…è®­ç»ƒä¸­æ–­ï¼‰
            print(f"Warning: Image {img_id} not found, using placeholder")
            image = Image.new('RGB', (224, 224), color='gray')

        if self.transform:
            image = self.transform(image)

        return image, label

def create_model(model_name='efficientnet_b0', num_classes=7, pretrained=True):
    """
    åˆ›å»ºæ¨¡å‹

    Args:
        model_name: æ¨¡å‹åç§°
            - 'efficientnet_b0': EfficientNet-B0 (æ¨è)
            - 'resnet18': ResNet18
            - 'resnet34': ResNet34 (æ›´å¤§ä½†æ›´å‡†ç¡®)
        num_classes: åˆ†ç±»æ•°é‡
        pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    """

    print(f"ğŸ“¦ åˆ›å»ºæ¨¡å‹: {model_name}")

  # ä½¿ç”¨ timm åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = timm.create_model(
        model_name,
        pretrained=pretrained,
        num_classes=num_classes
    )

    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    return model

def train_model(
    model_name='efficientnet_b0',
    epochs=10,
    batch_size=32,
    lr=0.001,
    device=None
):
    """è®­ç»ƒæ¨¡å‹"""

    print("="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒçš®è‚¤ç—…å˜åˆ†ç±»å™¨")
    print("="*60)
    print(f"ğŸ“‹ é…ç½®:")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   å­¦ä¹ ç‡: {lr}")
    print("="*60)

    # è®¾å¤‡ - æ”¯æŒ Apple Silicon
    if device is None:
        if torch.backends.mps.is_available():
            device = torch.device('mps')
            print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: Apple Silicon GPU (MPS)")
        elif torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: NVIDIA GPU (CUDA)")
        else:
            device = torch.device('cpu')
            print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: CPU")
    else:
        print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")

    if device.type == 'cpu':
        print("âš ï¸  ä½¿ç”¨ CPU è®­ç»ƒï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    # æ•°æ®è½¬æ¢
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomRotation(20),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    base_dir = Path("datasets/archive (6)")
    metadata = pd.read_csv(base_dir / "HAM10000_metadata.csv")

    print(f"âœ… æ€»æ ·æœ¬æ•°: {len(metadata)}")
    print(f"âœ… ç±»åˆ«åˆ†å¸ƒ:")
    for cls in DISEASE_CLASSES:
        count = len(metadata[metadata['dx'] == cls])
        print(f"   {DISEASE_NAMES[cls]}: {count}")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    from sklearn.model_selection import train_test_split
    train_df, val_df = train_test_split(
        metadata,
        test_size=0.2,
        stratify=metadata['dx'],
        random_state=42
    )

    print(f"\nâœ… è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_df)} æ ·æœ¬")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = HAM10000Dataset(
        train_df,
        base_dir / "HAM10000_images_part_1",
        base_dir / "HAM10000_images_part_2",
        transform=train_transform
    )

    val_dataset = HAM10000Dataset(
        val_df,
        base_dir / "HAM10000_images_part_1",
        base_dir / "HAM10000_images_part_2",
        transform=val_transform
    )

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # CPU æ¨¡å¼ä¸‹è®¾ä¸º 0
        pin_memory=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ“¦ åˆ›å»ºæ¨¡å‹...")
    model = create_model(model_name=model_name, num_classes=7, pretrained=True)
    model = model.to(device)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2
    )

    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'lr': []
    }

    best_val_acc = 0.0
    save_dir = Path("models/skin_lesion_classifier")
    save_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    print("\n" + "="*60)
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ")
    print("="*60)

    for epoch in range(epochs):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{epochs}")
        print("-" * 60)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        train_bar = tqdm(train_loader, desc="Training", ncols=100)
        for batch_idx, (images, labels) in enumerate(train_bar):
            images = images.to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            train_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*train_correct/train_total:.2f}%'
            })

        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            val_bar = tqdm(val_loader, desc="Validation", ncols=100)
            for images, labels in val_bar:
                images = images.to(device)
                labels = labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

                val_bar.set_postfix({
              'loss': f'{loss.item():.4f}',
           'acc': f'{100.*val_correct/val_total:.2f}%'
                })

        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]['lr']

        # è®°å½•å†å²
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)
        history['lr'].append(current_lr)

        print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"   è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"   éªŒè¯ - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"   å­¦ä¹ ç‡: {current_lr:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc

            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': avg_val_loss,
                'history': history,
                'class_names': DISEASE_CLASSES,
                'model_name': model_name
            }, save_dir / "best_model.pth")

            print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {save_dir / 'best_model.pth'}")
    print("="*60)

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = save_dir / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    print(f"ğŸ“ˆ è®­ç»ƒå†å²ä¿å­˜åœ¨: {history_path}")

    return model, history, best_val_acc

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='è®­ç»ƒçš®è‚¤ç—…å˜åˆ†ç±»å™¨')
    parser.add_argument('--model', type=str, default='efficientnet_b0',
                        choices=['efficientnet_b0', 'resnet18', 'resnet34'],
                        help='æ¨¡å‹åç§°')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')

    args = parser.parse_args()

    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ¨¡å‹: {args.model}")
    print(f"   è½®æ•°: {args.epochs}")
    print(f"   æ‰¹æ¬¡: {args.batch_size}")
    print(f"   å­¦ä¹ ç‡: {args.lr}\n")

    model, history, best_acc = train_model(
        model_name=args.model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr
    )

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
