"""
è®­ç»ƒç»“æœå¯è§†åŒ–å’Œæ€§èƒ½åˆ†æ
ç”Ÿæˆè®­ç»ƒæ›²çº¿ã€æ··æ·†çŸ©é˜µã€æ€§èƒ½æŠ¥å‘Šç­‰
"""

import torch
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import label_binarize
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“ - ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
import platform
system = platform.system()
if system == 'Darwin':  # macOS
    plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti TC', 'STHeiti', 'Arial Unicode MS']
elif system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'SimSun']
else:  # Linux
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']

plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")

# ç–¾ç—…ç±»åˆ«
DISEASE_CLASSES = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']
DISEASE_NAMES = {
    'akiec': 'å…‰åŒ–æ€§è§’åŒ–ç—…',
    'bcc': 'åŸºåº•ç»†èƒç™Œ',
    'bkl': 'è‰¯æ€§è§’åŒ–ç—…å˜',
    'df': 'çš®è‚¤çº¤ç»´ç˜¤',
    'mel': 'é»‘è‰²ç´ ç˜¤',
    'nv': 'é»‘è‰²ç´ ç—£',
    'vasc': 'è¡€ç®¡ç—…å˜'
}

def plot_training_curves(history, output_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    epochs = range(1, len(history['train_loss']) + 1)

    # åˆ›å»º 2x2 å­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–', fontsize=20, fontweight='bold', y=0.995)

    # 1. æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, history['train_loss'], 'b-o', label='è®­ç»ƒæŸå¤±', linewidth=2, markersize=6)
    axes[0, 0].plot(epochs, history['val_loss'], 'r-s', label='éªŒè¯æŸå¤±', linewidth=2, markersize=6)
    axes[0, 0].set_xlabel('Epoch', fontsize=12)
    axes[0, 0].set_ylabel('Loss', fontsize=12)
    axes[0, 0].set_title('æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3)

    # 2. å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(epochs, history['train_acc'], 'b-o', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, markersize=6)
    axes[0, 1].plot(epochs, history['val_acc'], 'r-s', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2, markersize=6)
    axes[0, 1].set_xlabel('Epoch', fontsize=12)
    axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)
    axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    axes[0, 1].legend(fontsize=11)
    axes[0, 1].grid(True, alpha=0.3)

    # 3. å­¦ä¹ ç‡å˜åŒ–
    if 'lr' in history and history['lr']:
        axes[1, 0].plot(epochs, history['lr'], 'g-d', linewidth=2, markersize=6)
        axes[1, 0].set_xlabel('Epoch', fontsize=12)
        axes[1, 0].set_ylabel('Learning Rate', fontsize=12)
        axes[1, 0].set_title('å­¦ä¹ ç‡å˜åŒ–', fontsize=14, fontweight='bold')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
    else:
        axes[1, 0].text(0.5, 0.5, 'æ— å­¦ä¹ ç‡æ•°æ®', ha='center', va='center', fontsize=14)
        axes[1, 0].axis('off')

    # 4. è®­ç»ƒ vs éªŒè¯å¯¹æ¯”
    x = np.arange(len(epochs))
    width = 0.35

    axes[1, 1].bar(x - width/2, history['train_acc'], width, label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.8)
    axes[1, 1].bar(x + width/2, history['val_acc'], width, label='éªŒè¯å‡†ç¡®ç‡', alpha=0.8)
    axes[1, 1].set_xlabel('Epoch', fontsize=12)
    axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)
    axes[1, 1].set_title('è®­ç»ƒ vs éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels([f'{e}' for e in epochs])
    axes[1, 1].legend(fontsize=11)
    axes[1, 1].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    # ä¿å­˜
    save_path = output_dir / 'training_curves.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
    plt.close()

def plot_confusion_matrix(y_true, y_pred, output_dir):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""

    output_dir = Path(output_dir)

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    # å½’ä¸€åŒ–
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

    # 1. åŸå§‹æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=[DISEASE_NAMES[c] for c in DISEASE_CLASSES],
           yticklabels=[DISEASE_NAMES[c] for c in DISEASE_CLASSES],
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    ax1.set_title('æ··æ·†çŸ©é˜µ (åŸå§‹æ•°é‡)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    ax1.set_ylabel('çœŸå®ç±»åˆ«', fontsize=12)

    # 2. å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', ax=ax2,
                xticklabels=[DISEASE_NAMES[c] for c in DISEASE_CLASSES],
                yticklabels=[DISEASE_NAMES[c] for c in DISEASE_CLASSES],
                cbar_kws={'label': 'æ¯”ä¾‹'})
    ax2.set_title('æ··æ·†çŸ©é˜µ (å½’ä¸€åŒ–)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    ax2.set_ylabel('çœŸå®ç±»åˆ«', fontsize=12)

    plt.tight_layout()

    # ä¿å­˜
    save_path = output_dir / 'confusion_matrix.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    plt.close()

    return cm

def plot_per_class_metrics(y_true, y_pred, output_dir):
    """ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æŒ‡æ ‡"""

    output_dir = Path(output_dir)

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, target_names=[DISEASE_NAMES[c] for c in DISEASE_CLASSES], output_dict=True)

    # æå–æŒ‡æ ‡
    classes = [DISEASE_NAMES[c] for c in DISEASE_CLASSES]
    precision = [report[c]['precision'] for c in classes]
    recall = [report[c]['recall'] for c in classes]
    f1 = [report[c]['f1-score'] for c in classes]
    support = [report[c]['support'] for c in classes]

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡', fontsize=20, fontweight='bold')

    x = np.arange(len(classes))
    width = 0.25

    # 1. Precision, Recall, F1-Score å¯¹æ¯”
    axes[0, 0].bar(x - width, precision, width, label='Precision', alpha=0.8)
    axes[0, 0].bar(x, recall, width, label='Recall', alpha=0.8)
    axes[0, 0].bar(x + width, f1, width, label='F1-Score', alpha=0.8)
    axes[0, 0].set_xlabel('ç–¾ç—…ç±»åˆ«', fontsize=12)
    axes[0, 0].set_ylabel('åˆ†æ•°', fontsize=12)
    axes[0, 0].set_title('Precision, Recall, F1-Score å¯¹æ¯”', fontsize=14, fontweight='bold')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(classes, rotation=45, ha='right')
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    axes[0, 0].set_ylim([0, 1.1])

    # 2. æ ·æœ¬æ•°é‡åˆ†å¸ƒ
    axes[0, 1].bar(x, support, alpha=0.8, color='skyblue')
    axes[0, 1].set_xlabel('ç–¾ç—…ç±»åˆ«', fontsize=12)
    axes[0, 1].set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
    axes[0, 1].set_title('éªŒè¯é›†æ ·æœ¬åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(classes, rotation=45, ha='right')
    axes[0, 1].grid(True, alpha=0.3, axis='y')

    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, v in enumerate(support):
        axes[0, 1].text(i, v + max(support)*0.02, str(int(v)), ha='center', va='bottom')

    # 3. F1-Score æ’å
    f1_sorted_idx = np.argsort(f1)[::-1]
    axes[1, 0].barh([classes[i] for i in f1_sorted_idx], [f1[i] for i in f1_sorted_idx], alpha=0.8, color='lightgreen')
    axes[1, 0].set_xlabel('F1-Score', fontsize=12)
    axes[1, 0].set_title('F1-Score æ’å', fontsize=14, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3, axis='x')
    axes[1, 0].set_xlim([0, 1.1])

    # 4. å‡†ç¡®ç‡çƒ­åŠ›å›¾
    accuracy_per_class = [report[c]['recall'] for c in classes]  # Recall å³è¯¥ç±»çš„å‡†ç¡®ç‡
    accuracy_matrix = np.array(accuracy_per_class).reshape(1, -1)

    sns.heatmap(accuracy_matrix, annot=True, fmt='.2%', cmap='RdYlGn', ax=axes[1, 1],
                xticklabels=classes, yticklabels=['å‡†ç¡®ç‡'],
                cbar_kws={'label': 'å‡†ç¡®ç‡'}, vmin=0, vmax=1)
    axes[1, 1].set_title('å„ç±»åˆ«å‡†ç¡®ç‡çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
    axes[1, 1].set_xticklabels(classes, rotation=45, ha='right')

    plt.tight_layout()

    # ä¿å­˜
    save_path = output_dir / 'per_class_metrics.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å„ç±»åˆ«æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
    plt.close()

    return report

def generate_performance_report(history, report, cm, output_dir):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""

    output_dir = Path(output_dir)

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    best_epoch = np.argmax(history['val_acc']) + 1
    best_val_acc = max(history['val_acc'])
    final_train_acc = history['train_acc'][-1]
    final_val_acc = history['val_acc'][-1]

    # è¿‡æ‹Ÿåˆåˆ†æ
    overfit_gap = final_train_acc - final_val_acc

    # ç”ŸæˆæŠ¥å‘Š
    report_text = f"""# æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š

## è®­ç»ƒæ¦‚å†µ

- **è®­ç»ƒè½®æ•°**: {len(history['train_loss'])} epochs
- **æœ€ä½³ Epoch**: {best_epoch}
- **æœ€ä½³éªŒè¯å‡†ç¡®ç‡**: {best_val_acc:.2f}%

## æœ€ç»ˆæ€§èƒ½

- **è®­ç»ƒå‡†ç¡®ç‡**: {final_train_acc:.2f}%
- **éªŒè¯å‡†ç¡®ç‡**: {final_val_acc:.2f}%
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: {overfit_gap:.2f}% {'(è½»å¾®)' if overfit_gap < 5 else '(ä¸­ç­‰)' if overfit_gap < 10 else '(ä¸¥é‡)'}

## å„ç±»åˆ«æ€§èƒ½

| ç±»åˆ« | Precision | Recall | F1-Score | æ ·æœ¬æ•° |
|------|-----------|--------|----------|--------|
"""

    for cls in DISEASE_CLASSES:
        cls_name = DISEASE_NAMES[cls]
        if cls_name in report:
            p = report[cls_name]['precision']
            r = report[cls_name]['recall']
            f1 = report[cls_name]['f1-score']
            s = int(report[cls_name]['support'])
            report_text += f"| {cls_name} | {p:.4f} | {r:.4f} | {f1:.4f} | {s} |\n"

    report_text += f"""
## æ€»ä½“æŒ‡æ ‡

- **å®å¹³å‡ Precision**: {report['macro avg']['precision']:.4f}
- **å®å¹³å‡ Recall**: {report['macro avg']['recall']:.4f}
- **å®å¹³å‡ F1-Score**: {report['macro avg']['f1-score']:.4f}
- **åŠ æƒå¹³å‡ F1-Score**: {report['weighted avg']['f1-score']:.4f}

## æ··æ·†çŸ©é˜µåˆ†æ

### æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹

"""

    # æ‰¾å‡ºæœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    np.fill_diagonal(cm_normalized, 0)  # å¿½ç•¥å¯¹è§’çº¿

    confusion_pairs = []
    for i in range(len(DISEASE_CLASSES)):
        for j in range(len(DISEASE_CLASSES)):
            if i != j and cm_normalized[i, j] > 0.1:  # è¶…è¿‡10%çš„æ··æ·†
                confusion_pairs.append((
                    DISEASE_NAMES[DISEASE_CLASSES[i]],
                    DISEASE_NAMES[DISEASE_CLASSES[j]],
                    cm_normalized[i, j]
                ))

    confusion_pairs.sort(key=lambda x: x[2], reverse=True)

    for true_cls, pred_cls, ratio in confusion_pairs[:5]:
        report_text += f"- **{true_cls}** è¢«è¯¯åˆ¤ä¸º **{pred_cls}**: {ratio:.2%}\n"

    report_text += f"""
## æ¨¡å‹ä¼˜åŠ¿

"""

    # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„ç±»åˆ«
    f1_scores = [(DISEASE_NAMES[cls], report[DISEASE_NAMES[cls]]['f1-score'])
                 for cls in DISEASE_CLASSES if DISEASE_NAMES[cls] in report]
    f1_scores.sort(key=lambda x: x[1], reverse=True)

    for cls_name, f1 in f1_scores[:3]:
        report_text += f"- **{cls_name}**: F1-Score = {f1:.4f}\n"

    report_text += f"""
## æ”¹è¿›å»ºè®®

"""

    # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
    if overfit_gap > 10:
        report_text += "- âš ï¸ å­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ•°æ®å¢å¼ºæˆ–æ­£åˆ™åŒ–\n"

    if final_val_acc < 70:
        report_text += "- âš ï¸ æ•´ä½“å‡†ç¡®ç‡åä½ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡\n"

    # æ‰¾å‡ºè¡¨ç°å·®çš„ç±»åˆ«
    poor_classes = [(cls_name, f1) for cls_name, f1 in f1_scores if f1 < 0.7]
    if poor_classes:
        report_text += f"- âš ï¸ ä»¥ä¸‹ç±»åˆ«è¡¨ç°è¾ƒå·®ï¼Œå»ºè®®å¢åŠ æ ·æœ¬æˆ–ä½¿ç”¨ç±»åˆ«æƒé‡:\n"
        for cls_name, f1 in poor_classes:
            report_text += f"  - {cls_name}: F1-Score = {f1:.4f}\n"

    if not poor_classes and overfit_gap < 5 and final_val_acc > 80:
        report_text += "- âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œå„é¡¹æŒ‡æ ‡å‡è¡¡\n"

    report_text += f"""
---

**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

    # ä¿å­˜æŠ¥å‘Š
    report_path = output_dir / 'performance_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    return report_text

def plot_dataset_distribution(metadata_path, output_dir):
    """ç»˜åˆ¶æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ"""

    output_dir = Path(output_dir)

    print("\nğŸ“Š ç»˜åˆ¶æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ...")

    # åŠ è½½å…ƒæ•°æ®
    metadata = pd.read_csv(metadata_path)

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('HAM10000 æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒåˆ†æ', fontsize=18, fontweight='bold')

    # 1. ç–¾ç—…ç±»åˆ«åˆ†å¸ƒ - æŸ±çŠ¶å›¾
    ax1 = axes[0, 0]
    disease_counts = metadata['dx'].value_counts()
    disease_counts = disease_counts.reindex(DISEASE_CLASSES)

    colors = plt.cm.Set3(range(len(DISEASE_CLASSES)))
    bars = ax1.bar(range(len(DISEASE_CLASSES)), disease_counts.values, color=colors, edgecolor='black', linewidth=1.5)
    ax1.set_xlabel('ç–¾ç—…ç±»åˆ«', fontsize=12)
    ax1.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
    ax1.set_title('ç–¾ç—…ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(len(DISEASE_CLASSES)))
    ax1.set_xticklabels([DISEASE_NAMES[cls] for cls in DISEASE_CLASSES], rotation=45, ha='right')
    ax1.grid(True, alpha=0.3, axis='y')

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

    # 2. ç–¾ç—…ç±»åˆ«åˆ†å¸ƒ - é¥¼å›¾
    ax2 = axes[0, 1]
    explode = [0.05 if i == disease_counts.values.argmax() else 0 for i in range(len(DISEASE_CLASSES))]
    wedges, texts, autotexts = ax2.pie(disease_counts.values,
                                        labels=[DISEASE_NAMES[cls] for cls in DISEASE_CLASSES],
                                        autopct='%1.1f%%',
                                        colors=colors,
                                        explode=explode,
                                        startangle=90,
                                        textprops={'fontsize': 9})

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')

    ax2.set_title('ç–¾ç—…ç±»åˆ«å æ¯”åˆ†å¸ƒ', fontsize=14, fontweight='bold')

    # 3. ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
    ax3 = axes[1, 0]
    sorted_counts = disease_counts.sort_values(ascending=True)
    y_pos = np.arange(len(sorted_counts))

    bars = ax3.barh(y_pos, sorted_counts.values, color=colors, edgecolor='black', linewidth=1.5)
    ax3.set_yticks(y_pos)
    ax3.set_yticklabels([DISEASE_NAMES[cls] for cls in sorted_counts.index])
    ax3.set_xlabel('æ ·æœ¬æ•°é‡', fontsize=12)
    ax3.set_title('ç±»åˆ«ä¸å¹³è¡¡åˆ†æï¼ˆä»å°‘åˆ°å¤šï¼‰', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax3.text(width, bar.get_y() + bar.get_height()/2.,
                f' {int(width)}',
                ha='left', va='center', fontsize=10, fontweight='bold')

    # 4. ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
    ax4 = axes[1, 1]
    ax4.axis('off')

    total_samples = len(metadata)
    max_class = disease_counts.idxmax()
    min_class = disease_counts.idxmin()
    imbalance_ratio = disease_counts.max() / disease_counts.min()

    stats_text = f"""
æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

æ€»æ ·æœ¬æ•°: {total_samples:,}
ç±»åˆ«æ•°é‡: {len(DISEASE_CLASSES)}

æœ€å¤šç±»åˆ«: {DISEASE_NAMES[max_class]}
æ ·æœ¬æ•°: {disease_counts[max_class]}

æœ€å°‘ç±»åˆ«: {DISEASE_NAMES[min_class]}
æ ·æœ¬æ•°: {disease_counts[min_class]}

ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1
å¹³å‡æ ·æœ¬æ•°: {total_samples/len(DISEASE_CLASSES):.0f}
æ ·æœ¬æ•°ä¸­ä½æ•°: {disease_counts.median():.0f}
    """

    ax4.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',
            family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    save_path = output_dir / 'dataset_distribution.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… æ•°æ®é›†åˆ†å¸ƒå›¾è¡¨å·²ä¿å­˜: {save_path}")

    plt.close()

    return disease_counts

def visualize_training_results(model_dir='models/skin_lesion_classifier', dataset_dir='datasets/archive (6)'):
    """å¯è§†åŒ–è®­ç»ƒç»“æœ"""

    model_dir = Path(model_dir)
    dataset_dir = Path(dataset_dir)

    print("="*60)
    print("ğŸ“Š å¼€å§‹ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–")
    print("="*60)

    # åŠ è½½è®­ç»ƒå†å²
    history_path = model_dir / 'training_history.json'
    if not history_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå†å²: {history_path}")
        return

    with open(history_path, 'r') as f:
        history = json.load(f)

    print(f"âœ… åŠ è½½è®­ç»ƒå†å²: {len(history['train_loss'])} epochs")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = model_dir / 'visualizations'
    output_dir.mkdir(exist_ok=True)

    # 1. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿...")
    plot_training_curves(history, output_dir)

    # 2. ç»˜åˆ¶æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ
    metadata_path = dataset_dir / "HAM10000_metadata.csv"
    if metadata_path.exists():
        disease_counts = plot_dataset_distribution(metadata_path, output_dir)

        # 3. åˆ›å»ºç»¼åˆæ€»ç»“
        print("\nğŸ“Š ç”Ÿæˆç»¼åˆæ€»ç»“...")
        create_comprehensive_summary(history, disease_counts, output_dir)
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ•°æ®é›†å…ƒæ•°æ®: {metadata_path}")

    # 4. åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ˆå¦‚æœéœ€è¦æ··æ·†çŸ©é˜µï¼‰
    checkpoint_path = model_dir / 'best_model.pth'
    if checkpoint_path.exists():
        print("\nğŸ“¦ æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        print("âš ï¸  è¯¦ç»†è¯„ä¼°ï¼ˆæ··æ·†çŸ©é˜µï¼‰éœ€è¦é‡æ–°åŠ è½½æ•°æ®ï¼Œå·²è·³è¿‡")
        print("   å¦‚éœ€ç”Ÿæˆï¼Œè¯·è¿è¡Œå®Œæ•´è¯„ä¼°è„šæœ¬")

    print("\n" + "="*60)
    print("âœ… å¯è§†åŒ–å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("\nç”Ÿæˆçš„å›¾è¡¨:")
    print("  1. training_curves.png - è®­ç»ƒæ›²çº¿")
    print("  2. dataset_distribution.png - æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ")
    print("  3. comprehensive_summary.png - ç»¼åˆæ€»ç»“")
    print("="*60)

def create_comprehensive_summary(history, disease_counts, output_dir):
    """åˆ›å»ºç»¼åˆæ€»ç»“å›¾è¡¨"""

    output_dir = Path(output_dir)

    fig = plt.figure(figsize=(16, 10))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    fig.suptitle('HAM10000 çš®è‚¤ç—…å˜åˆ†ç±»å™¨ - ç»¼åˆåˆ†ææŠ¥å‘Š', fontsize=18, fontweight='bold')

    # è®¡ç®—å…³é”®æŒ‡æ ‡
    best_val_acc = max(history['val_acc'])
    best_epoch = history['val_acc'].index(best_val_acc) + 1
    total_samples = disease_counts.sum()

    # 1. æœ€ä½³éªŒè¯å‡†ç¡®ç‡å¡ç‰‡
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.axis('off')
    ax1.text(0.5, 0.7, f'{best_val_acc:.2f}%',
            ha='center', va='center', fontsize=42, fontweight='bold', color='#4CAF50')
    ax1.text(0.5, 0.3, 'æœ€ä½³éªŒè¯å‡†ç¡®ç‡',
            ha='center', va='center', fontsize=13, color='#666')
    ax1.add_patch(plt.Rectangle((0.05, 0.05), 0.9, 0.9, fill=False,
                                edgecolor='#4CAF50', linewidth=3))

    # 2. æœ€ä½³è½®æ•°å¡ç‰‡
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')
    ax2.text(0.5, 0.7, f'{best_epoch}',
            ha='center', va='center', fontsize=42, fontweight='bold', color='#2196F3')
    ax2.text(0.5, 0.3, 'æœ€ä½³è®­ç»ƒè½®æ•°',
            ha='center', va='center', fontsize=13, color='#666')
    ax2.add_patch(plt.Rectangle((0.05, 0.05), 0.9, 0.9, fill=False,
                                edgecolor='#2196F3', linewidth=3))

    # 3. æ•°æ®é›†è§„æ¨¡å¡ç‰‡
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.axis('off')
    ax3.text(0.5, 0.7, f'{total_samples:,}',
            ha='center', va='center', fontsize=42, fontweight='bold', color='#FF9800')
    ax3.text(0.5, 0.3, 'æ€»è®­ç»ƒæ ·æœ¬æ•°',
            ha='center', va='center', fontsize=13, color='#666')
    ax3.add_patch(plt.Rectangle((0.05, 0.05), 0.9, 0.9, fill=False,
                                edgecolor='#FF9800', linewidth=3))

    # 4. ç±»åˆ«åˆ†å¸ƒæ¨ªå‘æŸ±çŠ¶å›¾
    ax4 = fig.add_subplot(gs[1:, :])
    sorted_counts = disease_counts.sort_values(ascending=True)
    y_pos = np.arange(len(sorted_counts))

    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(sorted_counts)))
    bars = ax4.barh(y_pos, sorted_counts.values, color=colors, edgecolor='black', linewidth=2)

    ax4.set_yticks(y_pos)
    ax4.set_yticklabels([f'{DISEASE_NAMES[cls]}\n({cls})' for cls in sorted_counts.index],
                        fontsize=11)
    ax4.set_xlabel('æ ·æœ¬æ•°é‡', fontsize=13, fontweight='bold')
    ax4.set_title('å„ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ', fontsize=14, fontweight='bold', pad=15)
    ax4.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œç™¾åˆ†æ¯”
    for i, bar in enumerate(bars):
        width = bar.get_width()
        percentage = (width / total_samples) * 100
        ax4.text(width, bar.get_y() + bar.get_height()/2.,
                f'  {int(width)} ({percentage:.1f}%)',
                ha='left', va='center', fontsize=11, fontweight='bold')

    # æ·»åŠ å¹³å‡çº¿
    avg_samples = total_samples / len(DISEASE_CLASSES)
    ax4.axvline(avg_samples, color='red', linestyle='--', linewidth=2,
               label=f'å¹³å‡å€¼: {avg_samples:.0f}', alpha=0.7)
    ax4.legend(fontsize=11, loc='lower right')

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    save_path = output_dir / 'comprehensive_summary.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ç»¼åˆæ€»ç»“å›¾è¡¨å·²ä¿å­˜: {save_path}")

    plt.close()

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='å¯è§†åŒ–è®­ç»ƒç»“æœ')
    parser.add_argument('--model-dir', type=str, default='models/skin_lesion_classifier',
                        help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--dataset-dir', type=str, default='datasets/archive (6)',
                        help='æ•°æ®é›†ç›®å½•')

    args = parser.parse_args()

    visualize_training_results(args.model_dir, args.dataset_dir)
